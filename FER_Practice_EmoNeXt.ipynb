{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2e83f68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "클래스 목록: ['angry', 'disgust', 'fear', 'happy', 'neutral', 'sad', 'surprise']\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import classification_report\n",
    "import os\n",
    "\n",
    "# 하이퍼파라미터\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 10\n",
    "LEARNING_RATE = 0.001\n",
    "NUM_CLASSES = 7\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=1),  # FER2013은 흑백\n",
    "    transforms.Resize((48, 48)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "])\n",
    "\n",
    "train_data = ImageFolder(root=r'C:\\Users\\hyi8402\\Desktop\\실시간 얼굴 감정 검출\\DataSet\\FER2013\\train', transform=transform)\n",
    "test_data = ImageFolder(root=r'C:\\Users\\hyi8402\\Desktop\\실시간 얼굴 감정 검출\\DataSet\\FER2013\\test', transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# 클래스 확인\n",
    "print(\"클래스 목록:\", train_data.classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c589f8c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 48, 48])\n",
      "tensor(3)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "images, labels = next(iter(train_loader))\n",
    "\n",
    "print(images[0].shape)      # torch.Size([1, 48, 48])\n",
    "print(labels[0])            # 예: tensor(3)\n",
    "plt.imshow(images[0].squeeze(0), cmap='gray')  # squeeze로 채널 제거\n",
    "plt.title(f\"Label: {labels[0].item()} ({train_data.classes[labels[0]]})\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7af4b478",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmotionCNN(nn.Module):\n",
    "    def __init__(self, num_classes=7):\n",
    "        super(EmotionCNN, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),  # 24x24\n",
    "\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),  # 12x12\n",
    "\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)   # 6x6\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(128 * 6 * 6, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "233d1774",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = EmotionCNN(num_classes=NUM_CLASSES).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5d8d6778",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 1.5948\n",
      "Epoch 2/10, Loss: 1.3152\n",
      "Epoch 3/10, Loss: 1.1899\n",
      "Epoch 4/10, Loss: 1.0883\n",
      "Epoch 5/10, Loss: 0.9864\n",
      "Epoch 6/10, Loss: 0.8953\n",
      "Epoch 7/10, Loss: 0.7954\n",
      "Epoch 8/10, Loss: 0.6933\n",
      "Epoch 9/10, Loss: 0.6006\n",
      "Epoch 10/10, Loss: 0.5201\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS}, Loss: {total_loss/len(train_loader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "83014e55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 59.42%\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "print(f\"Test Accuracy: {100 * correct / total:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e487708d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       angry       0.45      0.56      0.50       958\n",
      "     disgust       0.86      0.45      0.59       111\n",
      "        fear       0.49      0.44      0.46      1024\n",
      "       happy       0.80      0.78      0.79      1774\n",
      "     neutral       0.54      0.53      0.53      1233\n",
      "         sad       0.45      0.46      0.46      1247\n",
      "    surprise       0.79      0.74      0.76       831\n",
      "\n",
      "    accuracy                           0.59      7178\n",
      "   macro avg       0.63      0.57      0.58      7178\n",
      "weighted avg       0.60      0.59      0.60      7178\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(all_labels, all_preds, target_names=train_data.classes))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c755c3d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LR 스케쥴러 사용\n",
    "\n",
    "import math\n",
    "import torch\n",
    "from torch.optim.lr_scheduler import LRScheduler\n",
    "\n",
    "class CosineAnnealing_WarmRestartsLR(LRScheduler):\n",
    "    def __init__(\n",
    "        self,\n",
    "        optimizer: torch.optim.Optimizer, #사용할 최적화 함수\n",
    "        warmup_steps: int = 128, #초기단계 스텝\n",
    "        cycle_steps: int = 512, #한 사이클에 들어가는 전체 스탭 수\n",
    "        min_lr: float = 0, #최소 학습률\n",
    "        max_lr: float = 1e-3, #최대 학습률\n",
    "    ): #초기 128스텝 까지 증가하고 이후 512 스텝마다 코사인 주기로 반복\n",
    "        self.optimizer = optimizer\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.cycle_steps = cycle_steps\n",
    "        self.min_lr = min_lr\n",
    "        self.max_lr = max_lr\n",
    "        \n",
    "        self.steps_counter = 0 #학습 중 스텝이 얼마나 진행되었는지 저장하는 변수\n",
    "        \n",
    "        super().__init__(optimizer) #부모 클래스인 LRS의 초기화 함수 호출\n",
    "        \n",
    "    \n",
    "    def step(self,epoch = None):\n",
    "        self.steps_counter+=1\n",
    "        \n",
    "        current_cyclestep =  self.steps_counter % self.cycle_steps #매 스탭마다 현재 스탭을 확인\n",
    "        \n",
    "        if (current_cyclestep < self.warmup_steps):\n",
    "            current_lr = self.min_lr \n",
    "            + (self.max_lr - self.min_lr) * (current_cyclestep / self.warmup_steps)\n",
    "        # 초기 스탭일 경우에는 학습률이 점차 크게 변화\n",
    "        \n",
    "        else:\n",
    "            current_lr = current_lr = self.min_lr \n",
    "            + (self.max_lr - self.min_lr) *(\n",
    "                1 + math.cos(math.pi * (current_cyclestep - self.warmup_steps)\n",
    "                / (self.cycle_steps - self.warmup_steps)\n",
    "                )) / 2\n",
    "        #cosine annealing 적용 : 부드럽게 감소시켜 최 저점까지 적용시킨다\n",
    "        #cos이 0 ~ 180도 까지의 값을 가진다 == 1 ~ -1 : 스텝 초기는 1, 후기는 -1\n",
    "        # 1+ ~~ / 2를 통해 cos이 1이면 1+1/2 = 1, 0이면 1-1/2 = 0로 부드럽게 최저점에 도달할 수 있다\n",
    "    \n",
    "        for param_group in self.optimizer.param_groups:\n",
    "            param_group[\"lr\"] = current_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ff5783",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.hub import load_state_dict_from_url\n",
    "from torchvision.models import vgg16_bn, VGG16_BN_Weights\n",
    "from torchvision.ops import StochasticDepth\n",
    "\n",
    "model_urls = {\n",
    "    \"convnext_tiny_1k\": \"https://dl.fbaipublicfiles.com/convnext/convnext_tiny_1k_224_ema.pth\",\n",
    "    \"convnext_small_1k\": \"https://dl.fbaipublicfiles.com/convnext/convnext_small_1k_224_ema.pth\",\n",
    "    \"convnext_base_1k\": \"https://dl.fbaipublicfiles.com/convnext/convnext_base_1k_224_ema.pth\",\n",
    "    \"convnext_large_1k\": \"https://dl.fbaipublicfiles.com/convnext/convnext_large_1k_224_ema.pth\",\n",
    "    \"convnext_tiny_22k\": \"https://dl.fbaipublicfiles.com/convnext/convnext_tiny_22k_224.pth\",\n",
    "    \"convnext_small_22k\": \"https://dl.fbaipublicfiles.com/convnext/convnext_small_22k_224.pth\",\n",
    "    \"convnext_base_22k\": \"https://dl.fbaipublicfiles.com/convnext/convnext_base_22k_224.pth\",\n",
    "    \"convnext_large_22k\": \"https://dl.fbaipublicfiles.com/convnext/convnext_large_22k_224.pth\",\n",
    "    \"convnext_xlarge_22k\": \"https://dl.fbaipublicfiles.com/convnext/convnext_xlarge_22k_224.pth\",\n",
    "}\n",
    "\n",
    "class SE_Layer(nn.Module): #Squeeze and Excitation 층\n",
    "    def __init__(self, channel, reduction = 16): #입력  태널 수와 축소 비율을 받아와서 초기화\n",
    "        super(SE_Layer, self).__init__()\n",
    "        \n",
    "        self.avg_pool = nn.AdaptiveMaxPool2d(1) #B,C,H,W를 B,C,1,1로 만든다. (1인 이유는 인자 값)\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(channel,channel // reduction, bias=False),\n",
    "            #chennel 에서 chennel / reduction 로 채널 수 압축\n",
    "            nn.ReLU(inplace = True),\n",
    "            #비선형 활성화\n",
    "            nn.Linear(channel//reduction, channel, bias = False),\n",
    "            # channel / reduction 에서 channel로 차원 복원\n",
    "            nn.Sigmoid(),\n",
    "            # 채널 중요도를 0 ~ 1 사이로 정규화\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        b, c, _, _ = x.size() #입력 이미지의 배치와 채널만 본다\n",
    "        y = self.avg_pool(x).view(b, c) #x를 채널별로 평균값만 추출하여 B,C로 변형 == 각 채널마다 대푯값 하나씩만 남김\n",
    "        y = self.fc(y).view(b,c,1,1) #fc 레이어를 통과시켜서 각 채널의 중요도(가중치)를 계산, 다시 B,C,1,1로 변형해서 곱셈 가늘\n",
    "        return x * y.expand_as(x) #입력 이미지 x에 채널별로 y를 곱셈 --- 중요도가 크면 강조, 작으면 억제\n",
    "\n",
    "# 그래서 얜 뭘하는 걸까?\n",
    "# 이미지를 입력 받아서 채널 별 평균으로 각 채널의 대푯값을 구하고, 그 값을 FC에 넣어서 채널별 가중치를 학습해\n",
    "# 중요한건 해당 가중치가 MLP 역전파를 통해 업데이트 되면서 점차 값이 올바르게 바뀌어 나간다는거\n",
    "# 아마 또 편미분하고 그 값 기반으로 업데이트 하고, 그냥 최적화 GD랑 동일한 과정일것으로 예상된다 \n",
    "\n",
    "class DotProductSelfAttention(nn.Module): #내적 점곱(DotProduct Self-Attention) 처리\n",
    "    def __init__(self, input_dim): # 입력 특징 차원 수를 입력으로 받아 초기화\n",
    "        super(DotProductSelfAttention, self).__init__()\n",
    "        self.input_dim = input_dim \n",
    "        self.norm = nn.LayerNorm(input_dim) #입력 x를 정규화 하는 메서드\n",
    "        self.query = nn.Linear(input_dim, input_dim) # Q(Query)를 만드는 선형 레이어\n",
    "        self.key = nn.Linear(input_dim, input_dim) # K(Key)를 만드는 선형 레이어\n",
    "        self.value = nn.Linear(input_dim, input_dim) # V(Value)를 만드는 선형 레이어\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.norm(x) #입력 x를 정규화\n",
    "        query = self.query(x) #정규화 한 X를 집어넣어 Q,K,V를 계산\n",
    "        key = self.key(x)\n",
    "        value = self.value(x)\n",
    "\n",
    "        scale = 1 / math.sqrt(math.sqrt(self.input_dim)) #입력 차원에 따라 정규화 (크기 조정)\n",
    "        scores = torch.matmul(query, key.transpose(-2, -1)) * scale #Q X K_t 계산 => 결과 : 각 위치간 유사도 행렬\n",
    "        #코드는그냥 마지막 2 층을 서로 바꿔버린다는 의미\n",
    "        attention_weights = torch.softmax(scores, dim=-1) #어텐션 가중치를 소프트맥스에 따라 정규화\n",
    "\n",
    "        attended_values = torch.matmul(attention_weights, value) #어텐션 가중치에 V를 곱하여 중요 정보를 반영한 결과 생성 \n",
    "        output = attended_values + x #잔차 연결\n",
    "\n",
    "        return output, attention_weights #어텐션 가중치값과 잔차연결 된 이미지 출력\n",
    "\n",
    "# 코드 자체로는 일반적인 어텐션이랑 다를게 없는 듯\n",
    "# 중요하게 판단할 점은 Q,K,V 가 선형 레이어를 거쳐서 만들어진다는거. 즉 학습된다\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    r\"\"\"LayerNorm that supports two data formats: channels_last (default) or channels_first.\n",
    "    The ordering of the dimensions in the inputs. channels_last corresponds to inputs with\n",
    "    shape (batch_size, height, width, channels) while channels_first corresponds to inputs\n",
    "    with shape (batch_size, channels, height, width).\n",
    "    \"\"\" # 해당 LayerNorm은 탠서가 마지막에 있는 포맷이든, (B,H,W,C) 앞에 있는 포맷이든 둘다 처리 가능하다\n",
    "\n",
    "    def __init__(self, normalized_shape, eps=1e-6, data_format=\"channels_last\"):\n",
    "        #정규화할 차원의 크기를 입력받으며 초기화 \n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(normalized_shape)) # 정규화 출력에 곱해질 가중치 설정 (초기값 1)\n",
    "        self.bias = nn.Parameter(torch.zeros(normalized_shape)) #정규화 출력에 더해질 바이어스 설정 (초기값 0)\n",
    "        self.eps = eps #매우 작은 크기의 숫자 (앱실론) 설정\n",
    "        self.data_format = data_format #입력 데이터 포맷 설정\n",
    "        if self.data_format not in [\"channels_last\", \"channels_first\"]:\n",
    "            # 정해진 입력 데이터 포맷을 따르지 않는 데이터가 입력되었을 경우\n",
    "            raise NotImplementedError #예외 발생\n",
    "        self.normalized_shape = (normalized_shape,) #튜플의 형태로 저장\n",
    "\n",
    "    def forward(self, x): #데이터 포맷의 형태가 Channels_last라면 \n",
    "        if self.data_format == \"channels_last\": # (N,H,W,C)\n",
    "            return F.layer_norm(\n",
    "                x, self.normalized_shape, self.weight, self.bias, self.eps\n",
    "            ) #Pytorch의 내장 F.Layer_Norm으로 정규화 수행\n",
    "        elif self.data_format == \"channels_first\": #Channel_First라면, (N,C,H,W)\n",
    "            u = x.mean(1, keepdim=True) #채널 방향 (dim =1)로 평균값 계산 : 2번째 차원이여서 1\n",
    "            s = (x - u).pow(2).mean(1, keepdim=True) #분산 계산\n",
    "            x = (x - u) / torch.sqrt(s + self.eps) #입력 데이터의 정규화 수행\n",
    "            x = self.weight[:, None, None] * x + self.bias[:, None, None] #가중치와 바이어스를 곱하고 더하기\n",
    "            return x\n",
    "\n",
    "    # 왜 따로 이렇게 만들었을까? Pytorch의 내장 함수 쓰지않고?\n",
    "    # 유지보수성이라든가, 성능 향상적 측면에서 효율적일 수 있기 때문\n",
    "    \n",
    "class Block(nn.Module):\n",
    "    r\"\"\"ConvNeXt Block. There are two equivalent implementations:\n",
    "    (1) DwConv -> LayerNorm (channels_first) -> 1x1 Conv -> GELU -> 1x1 Conv; all in (N, C, H, W)\n",
    "    (2) DwConv -> Permute to (N, H, W, C); LayerNorm (channels_last) -> Linear -> GELU -> Linear; Permute back\n",
    "    We use (2) as we find it slightly faster in PyTorch\n",
    "\n",
    "    Args:\n",
    "        dim (int): Number of input channels.\n",
    "        drop_path (float): Stochastic depth rate. Default: 0.0\n",
    "        layer_scale_init_value (float): Init value for Layer Scale. Default: 1e-6.\n",
    "    \"\"\"\n",
    "    #ConvNeXt 블록 : 2가지 방식으로 동일하게 구현이 가능하다.\n",
    "    #DwConv -> LayerNorm (Channels_First) -> 1x1 Conv -> GELU -> 1x1 Conv : 모든 연산을 (N,C,H,W)포맷에서 수행\n",
    "    #DwConv -> (N, H, W, C) 형태로 변환 -> LayerNorm (channels_last) -> Linear -> GELU -> Linear -> 다시 원래 형태로 변환\n",
    "    #2번 방식이 좀 더 빨라서 2번을 사용한다\n",
    "    \n",
    "    #궁금 1. 왜 2번은 Linear로 Conv로 대체했을까?\n",
    "    #1x1 Conv는 공간을 건드리지 않고 채널 간 변환만 하므로 (H,W)는 그대로 두되 C만 변환하는 Linear과 동일하게 작동할 수 있다\n",
    "\n",
    "    def __init__(self, dim, drop_path=0.0, layer_scale_init_value=1e-6):\n",
    "        super().__init__() \n",
    "        #dim = 입력 채널 수, Stochastic Depth 비율 : 블록을 확률적으로 생략하는 비율, Local_Scale 초기값\n",
    "        self.dwconv = nn.Conv2d(\n",
    "            dim, dim, kernel_size=7, padding=3, groups=dim\n",
    "        )  # depthwise conv : 채널마다 독립적인 Convolution 수행, 7x7커널을 사용하며 채널수는 유지, 출력 크기를 입력과 동일하게 유지\n",
    "        # Depthwise Conv란? Xception에서 제시되었던 컨셉의 Conv, 채널별로 독립적인 Conv연산을 수행한다\n",
    "        # 결과적인 채널 수는 동일하다\n",
    "        self.norm = LayerNorm(dim, eps=1e-6) # 입력 텐서의 채널 방향에 대해 레이어 정규화 수행 \n",
    "        self.pwconv1 = nn.Linear(\n",
    "            dim, 4 * dim\n",
    "        )  # pointwise/1x1 convs, implemented with linear layers : Pointwise 1x1 Conv 수행\n",
    "        self.act = nn.GELU() #비선형 활성화 GELU 적용\n",
    "        self.pwconv2 = nn.Linear(4 * dim, dim) #다시 Pointwise 1x1 Conv 수행 (원래대로 축소)\n",
    "        self.gamma = (\n",
    "            nn.Parameter(layer_scale_init_value * torch.ones((dim)), requires_grad=True)\n",
    "            if layer_scale_init_value > 0\n",
    "            else None\n",
    "        )#dim 차원만큼 각 채널에 Scale을 곱, 아주 작은 값에서 시작하여 학습하면서 커진다\n",
    "        #이는 잔차블록의 역할\n",
    "        self.stochastic_depth = StochasticDepth(drop_path, \"row\")\n",
    "        # drop_path의 확률에 따라 블록을 통째로 skip할 수 있다\n",
    "        #(= 배치 단위가 아닌 sample(행)단위로 drop 여부를 결정한다\n",
    "\n",
    "    def forward(self, x): # 순전파 기준\n",
    "        input = x # 입력 데이터 x\n",
    "        x = self.dwconv(x) # 1x1 Conv\n",
    "        x = x.permute(0, 2, 3, 1)  # (N, C, H, W) -> (N, H, W, C)\n",
    "        x = self.norm(x) \n",
    "        x = self.pwconv1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.pwconv2(x)\n",
    "        if self.gamma is not None: # \n",
    "            x = self.gamma * x\n",
    "        x = x.permute(0, 3, 1, 2)  # (N, H, W, C) -> (N, C, H, W)\n",
    "\n",
    "        x = input + self.stochastic_depth(x)\n",
    "        return x\n",
    "\n",
    "class EmoNeXt(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_chans=3, # 입력 이미지의 채널 수\n",
    "        num_classes=1000, #출력 클래스의 수 = 1000개\n",
    "        depths=None, # 각 stage에서의 블록의 개수를 정의\n",
    "        dims=None, # 각 stage에서의 차원 크기 리스트\n",
    "        drop_path_rate=0.0, # Stochastic Depth를 적용할 확률\n",
    "        layer_scale_init_value=1e-6, # LayersCALE 시의 초기값\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        if dims is None:\n",
    "            dims = [96, 192, 384, 768]\n",
    "        if depths is None:\n",
    "            depths = [3, 3, 9, 3]\n",
    "\n",
    "        # localization-network 정의\n",
    "        self.localization = nn.Sequential(\n",
    "            nn.Conv2d(3, 8, kernel_size=7),\n",
    "            nn.BatchNorm2d(8),\n",
    "            nn.MaxPool2d(2, stride=2),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(8, 10, kernel_size=5),\n",
    "            nn.BatchNorm2d(10),\n",
    "            nn.MaxPool2d(2, stride=2),\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    "\n",
    "        # Regressor for the 3 * 2 affine matrix\n",
    "        self.fc_loc = nn.Sequential(\n",
    "            nn.Linear(10 * 52 * 52, 32), nn.ReLU(True), nn.Linear(32, 3 * 2)\n",
    "        )\n",
    "        # STN 과정(Affine 변환)을 위한 변환 행렬 theta를 예측하는 Localization Head\n",
    "        \n",
    "        self.downsample_layers = (\n",
    "            nn.ModuleList()\n",
    "        )  # 여러 스테이지를 담을 모듈 리스트 선언\n",
    "        \n",
    "        stem = nn.Sequential(\n",
    "            nn.Conv2d(in_chans, dims[0], kernel_size=4, stride=4),\n",
    "            LayerNorm(dims[0], eps=1e-6, data_format=\"channels_first\"),\n",
    "        ) # 초기 해상도 감소 및 채널 정규화\n",
    "        \n",
    "        self.downsample_layers.append(stem) #stage로 추가\n",
    "        \n",
    "        for i in range(3):\n",
    "            downsample_layer = nn.Sequential(\n",
    "                LayerNorm(dims[i], eps=1e-6, data_format=\"channels_first\"), #이전 출력 정규화\n",
    "                nn.Conv2d(dims[i], dims[i + 1], kernel_size=2, stride=2), #채널 수 증가\n",
    "                SE_Layer(dims[i + 1]), #채널의 어텐션 매커니증 적용\n",
    "            )\n",
    "            self.downsample_layers.append(downsample_layer) #stage 추가\n",
    "\n",
    "        self.stages = (\n",
    "            nn.ModuleList()\n",
    "        )  #스테이지를 담을 모듈리스트를 선언 2\n",
    "        dp_rates = [x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))]\n",
    "        #전체 블록 수 만큼 Stochastic Depth 확률을 고르게 생성한다\n",
    "        # 깊은 블록일수록 더 자주 skip 되게 설정 -> 왜? 과적합 방지와 학습 안정화의 효력\n",
    "        cur = 0\n",
    "        for i in range(4):\n",
    "            stage = nn.Sequential(\n",
    "                *[\n",
    "                    Block(\n",
    "                        dim=dims[i], # 현재 stage의 채널 수\n",
    "                        drop_path=dp_rates[cur + j], # 현재 Block에 해당하는 drop_path 확률\n",
    "                        layer_scale_init_value=layer_scale_init_value, #초기 값\n",
    "                    )\n",
    "                    for j in range(depths[i]) #각 stage 별로 블록 개수 설정해서 stage 하나로 만든다\n",
    "                ]\n",
    "            )\n",
    "            self.stages.append(stage)\n",
    "            cur += depths[i] #다음 stage의 dp_rates 인덱스를 맞추기 위해 현재 Block 수만큼 더하기\n",
    "            \n",
    "        # 96채널 3개 192채널 3개 384채널 9개 768채널 3개 로 구설\n",
    "        # 점점 고차원의 특징을 학습한다    \n",
    "        \n",
    "        #최종 채널 축[-1]에 대해 정규화와 어텐션 수행\n",
    "        self.norm = nn.LayerNorm(dims[-1], eps=1e-6)  # final norm layer\n",
    "        self.attention = DotProductSelfAttention(dims[-1])\n",
    "        self.head = nn.Linear(dims[-1], num_classes)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, (nn.Conv2d, nn.Linear)):\n",
    "                nn.init.trunc_normal_(m.weight, std=0.02)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "                    \n",
    "        # 모델 내의 모든 서브 모듈을 순회하여 가중치 초기화와 바이어스 초기화 진행\n",
    "\n",
    "        self.fc_loc[2].weight.data.zero_()\n",
    "        self.fc_loc[2].bias.data.copy_(\n",
    "            torch.tensor([1, 0, 0, 0, 1, 0], dtype=torch.float)\n",
    "        # STN 변환을 위한 Affine 변환 행렬 초기화\n",
    "        # STN에서는 항등 행렬을 사용하여 처음에 아무 변화 없이 이미지가 전달되도록 보장된다\n",
    "        )\n",
    "        \n",
    "        #해당 초기화는 모델 실행 시에 한번만 실행되는 초기화 코드\n",
    "\n",
    "    def stn(self, x): #STN : 이미지를 적다하게 조절하여 잘 인식하게끔 만드는 과정\n",
    "        xs = self.localization(x) #Localization으로 특징맵을 추출\n",
    "        xs = xs.view(-1, 10 * 52 * 52) # 2D텐서로 펼침\n",
    "        theta = self.fc_loc(xs) # 변환행렬 Theta를 예측하여 대입\n",
    "        theta = theta.view(-1, 2, 3)\n",
    " \n",
    "        grid = F.affine_grid(theta, x.size(), align_corners=True) #STN을 위해 이미지를 변환시킬 픽셀위치좌표계 생성\n",
    "        x = F.grid_sample(x, grid, align_corners=True) # 만들어진 grid에 따라 입력 이미지 이동(샘플링 및 보간) \n",
    "\n",
    "        return x\n",
    "\n",
    "    def forward_features(self, x): #순방향 처리 과정\n",
    "        for i in range(4):\n",
    "            x = self.downsample_layers[i](x)\n",
    "            x = self.stages[i](x)\n",
    "        return self.norm(\n",
    "            x.mean([-2, -1]) \n",
    "        )  # global average pooling, (N, C, H, W) -> (N, C)\n",
    "        \n",
    "    # 최종 결과물은 특징벡터를 뽑아낸다\n",
    "\n",
    "    def forward(self, x, labels=None): #순방향\n",
    "        x = self.stn(x) #STN 과정\n",
    "        x = self.forward_features(x) #특징 추출\n",
    "        _, weights = self.attention(x) # 가중치 추출\n",
    "        logits = self.head(x) #확률기반 예측\n",
    "\n",
    "        if labels is not None: #정답이 있으면 Loss 계산\n",
    "            mean_attention_weight = torch.mean(weights)\n",
    "            attention_loss = torch.mean((weights - mean_attention_weight) ** 2)\n",
    "\n",
    "            loss = F.cross_entropy(logits, labels, label_smoothing=0.2) + attention_loss\n",
    "            return torch.argmax(logits, dim=1), logits, loss\n",
    "\n",
    "        return torch.argmax(logits, dim=1), logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19168f6a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fer-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
