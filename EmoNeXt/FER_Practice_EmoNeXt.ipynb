{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e83f68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "클래스 목록: ['angry', 'disgust', 'fear', 'happy', 'neutral', 'sad', 'surprise']\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import classification_report\n",
    "import os\n",
    "\n",
    "# 하이퍼파라미터\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 10\n",
    "LEARNING_RATE = 0.001\n",
    "NUM_CLASSES = 7\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=1),  # FER2013은 흑백\n",
    "    transforms.Resize((48, 48)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "])\n",
    "\n",
    "train_data = ImageFolder(root=r'C:\\Users\\hyi8402\\Desktop\\실시간 얼굴 감정 검출\\EmoNeXt실습\\DataSet\\FER2013\\train', transform=transform)\n",
    "test_data = ImageFolder(root=r'C:\\Users\\hyi8402\\Desktop\\실시간 얼굴 감정 검출\\EmoNeXt실습\\DataSet\\FER2013\\test', transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# 클래스 확인\n",
    "print(\"클래스 목록:\", train_data.classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c589f8c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 48, 48])\n",
      "tensor(3)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "images, labels = next(iter(train_loader))\n",
    "\n",
    "print(images[0].shape)      # torch.Size([1, 48, 48])\n",
    "print(labels[0])            # 예: tensor(3)\n",
    "plt.imshow(images[0].squeeze(0), cmap='gray')  # squeeze로 채널 제거\n",
    "plt.title(f\"Label: {labels[0].item()} ({train_data.classes[labels[0]]})\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7af4b478",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mEmotionCNN\u001b[39;00m(\u001b[43mnn\u001b[49m\u001b[38;5;241m.\u001b[39mModule):\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, num_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m7\u001b[39m):\n\u001b[0;32m      3\u001b[0m         \u001b[38;5;28msuper\u001b[39m(EmotionCNN, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "class EmotionCNN(nn.Module):\n",
    "    def __init__(self, num_classes=7):\n",
    "        super(EmotionCNN, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),  # 24x24\n",
    "\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),  # 12x12\n",
    "\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)   # 6x6\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(128 * 6 * 6, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "233d1774",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = EmotionCNN(num_classes=NUM_CLASSES).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5d8d6778",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 1.5948\n",
      "Epoch 2/10, Loss: 1.3152\n",
      "Epoch 3/10, Loss: 1.1899\n",
      "Epoch 4/10, Loss: 1.0883\n",
      "Epoch 5/10, Loss: 0.9864\n",
      "Epoch 6/10, Loss: 0.8953\n",
      "Epoch 7/10, Loss: 0.7954\n",
      "Epoch 8/10, Loss: 0.6933\n",
      "Epoch 9/10, Loss: 0.6006\n",
      "Epoch 10/10, Loss: 0.5201\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS}, Loss: {total_loss/len(train_loader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "83014e55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 59.42%\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "print(f\"Test Accuracy: {100 * correct / total:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e487708d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       angry       0.45      0.56      0.50       958\n",
      "     disgust       0.86      0.45      0.59       111\n",
      "        fear       0.49      0.44      0.46      1024\n",
      "       happy       0.80      0.78      0.79      1774\n",
      "     neutral       0.54      0.53      0.53      1233\n",
      "         sad       0.45      0.46      0.46      1247\n",
      "    surprise       0.79      0.74      0.76       831\n",
      "\n",
      "    accuracy                           0.59      7178\n",
      "   macro avg       0.63      0.57      0.58      7178\n",
      "weighted avg       0.60      0.59      0.60      7178\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(all_labels, all_preds, target_names=train_data.classes))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c755c3d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LR 스케쥴러 사용\n",
    "\n",
    "import math\n",
    "import torch\n",
    "from torch.optim.lr_scheduler import LRScheduler\n",
    "\n",
    "class CosineAnnealing_WarmRestartsLR(LRScheduler):\n",
    "    def __init__(\n",
    "        self,\n",
    "        optimizer: torch.optim.Optimizer, #사용할 최적화 함수\n",
    "        warmup_steps: int = 128, #초기단계 스텝\n",
    "        cycle_steps: int = 512, #한 사이클에 들어가는 전체 스탭 수\n",
    "        min_lr: float = 0, #최소 학습률\n",
    "        max_lr: float = 1e-3, #최대 학습률\n",
    "    ): #초기 128스텝 까지 증가하고 이후 512 스텝마다 코사인 주기로 반복\n",
    "        self.optimizer = optimizer\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.cycle_steps = cycle_steps\n",
    "        self.min_lr = min_lr\n",
    "        self.max_lr = max_lr\n",
    "        \n",
    "        self.steps_counter = 0 #학습 중 스텝이 얼마나 진행되었는지 저장하는 변수\n",
    "        \n",
    "        super().__init__(optimizer) #부모 클래스인 LRS의 초기화 함수 호출\n",
    "        \n",
    "    \n",
    "    def step(self,epoch = None):\n",
    "        self.steps_counter+=1\n",
    "        \n",
    "        current_cyclestep =  self.steps_counter % self.cycle_steps #매 스탭마다 현재 스탭을 확인\n",
    "        \n",
    "        if (current_cyclestep < self.warmup_steps):\n",
    "            current_lr = self.min_lr \n",
    "            + (self.max_lr - self.min_lr) * (current_cyclestep / self.warmup_steps)\n",
    "        # 초기 스탭일 경우에는 학습률이 점차 크게 변화\n",
    "        \n",
    "        else:\n",
    "            current_lr = current_lr = self.min_lr \n",
    "            + (self.max_lr - self.min_lr) *(\n",
    "                1 + math.cos(math.pi * (current_cyclestep - self.warmup_steps)\n",
    "                / (self.cycle_steps - self.warmup_steps)\n",
    "                )) / 2\n",
    "        #cosine annealing 적용 : 부드럽게 감소시켜 최 저점까지 적용시킨다\n",
    "        #cos이 0 ~ 180도 까지의 값을 가진다 == 1 ~ -1 : 스텝 초기는 1, 후기는 -1\n",
    "        # 1+ ~~ / 2를 통해 cos이 1이면 1+1/2 = 1, 0이면 1-1/2 = 0로 부드럽게 최저점에 도달할 수 있다\n",
    "    \n",
    "        for param_group in self.optimizer.param_groups:\n",
    "            param_group[\"lr\"] = current_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "40ff5783",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.hub import load_state_dict_from_url\n",
    "from torchvision.models import vgg16_bn, VGG16_BN_Weights\n",
    "from torchvision.ops import StochasticDepth\n",
    "\n",
    "model_urls = {\n",
    "    \"convnext_tiny_1k\": \"https://dl.fbaipublicfiles.com/convnext/convnext_tiny_1k_224_ema.pth\",\n",
    "    \"convnext_small_1k\": \"https://dl.fbaipublicfiles.com/convnext/convnext_small_1k_224_ema.pth\",\n",
    "    \"convnext_base_1k\": \"https://dl.fbaipublicfiles.com/convnext/convnext_base_1k_224_ema.pth\",\n",
    "    \"convnext_large_1k\": \"https://dl.fbaipublicfiles.com/convnext/convnext_large_1k_224_ema.pth\",\n",
    "    \"convnext_tiny_22k\": \"https://dl.fbaipublicfiles.com/convnext/convnext_tiny_22k_224.pth\",\n",
    "    \"convnext_small_22k\": \"https://dl.fbaipublicfiles.com/convnext/convnext_small_22k_224.pth\",\n",
    "    \"convnext_base_22k\": \"https://dl.fbaipublicfiles.com/convnext/convnext_base_22k_224.pth\",\n",
    "    \"convnext_large_22k\": \"https://dl.fbaipublicfiles.com/convnext/convnext_large_22k_224.pth\",\n",
    "    \"convnext_xlarge_22k\": \"https://dl.fbaipublicfiles.com/convnext/convnext_xlarge_22k_224.pth\",\n",
    "}\n",
    "\n",
    "class SE_Layer(nn.Module): #Squeeze and Excitation 층\n",
    "    def __init__(self, channel, reduction = 16): #입력  태널 수와 축소 비율을 받아와서 초기화\n",
    "        super(SE_Layer, self).__init__()\n",
    "        \n",
    "        self.avg_pool = nn.AdaptiveMaxPool2d(1) #B,C,H,W를 B,C,1,1로 만든다. (1인 이유는 인자 값)\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(channel,channel // reduction, bias=False),\n",
    "            #chennel 에서 chennel / reduction 로 채널 수 압축\n",
    "            nn.ReLU(inplace = True),\n",
    "            #비선형 활성화\n",
    "            nn.Linear(channel//reduction, channel, bias = False),\n",
    "            # channel / reduction 에서 channel로 차원 복원\n",
    "            nn.Sigmoid(),\n",
    "            # 채널 중요도를 0 ~ 1 사이로 정규화\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        b, c, _, _ = x.size() #입력 이미지의 배치와 채널만 본다\n",
    "        y = self.avg_pool(x).view(b, c) #x를 채널별로 평균값만 추출하여 B,C로 변형 == 각 채널마다 대푯값 하나씩만 남김\n",
    "        y = self.fc(y).view(b,c,1,1) #fc 레이어를 통과시켜서 각 채널의 중요도(가중치)를 계산, 다시 B,C,1,1로 변형해서 곱셈 가늘\n",
    "        return x * y.expand_as(x) #입력 이미지 x에 채널별로 y를 곱셈 --- 중요도가 크면 강조, 작으면 억제\n",
    "\n",
    "# 그래서 얜 뭘하는 걸까?\n",
    "# 이미지를 입력 받아서 채널 별 평균으로 각 채널의 대푯값을 구하고, 그 값을 FC에 넣어서 채널별 가중치를 학습해\n",
    "# 중요한건 해당 가중치가 MLP 역전파를 통해 업데이트 되면서 점차 값이 올바르게 바뀌어 나간다는거\n",
    "# 아마 또 편미분하고 그 값 기반으로 업데이트 하고, 그냥 최적화 GD랑 동일한 과정일것으로 예상된다 \n",
    "\n",
    "class DotProductSelfAttention(nn.Module): #내적 점곱(DotProduct Self-Attention) 처리\n",
    "    def __init__(self, input_dim): # 입력 특징 차원 수를 입력으로 받아 초기화\n",
    "        super(DotProductSelfAttention, self).__init__()\n",
    "        self.input_dim = input_dim \n",
    "        self.norm = nn.LayerNorm(input_dim) #입력 x를 정규화 하는 메서드\n",
    "        self.query = nn.Linear(input_dim, input_dim) # Q(Query)를 만드는 선형 레이어\n",
    "        self.key = nn.Linear(input_dim, input_dim) # K(Key)를 만드는 선형 레이어\n",
    "        self.value = nn.Linear(input_dim, input_dim) # V(Value)를 만드는 선형 레이어\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.norm(x) #입력 x를 정규화\n",
    "        query = self.query(x) #정규화 한 X를 집어넣어 Q,K,V를 계산\n",
    "        key = self.key(x)\n",
    "        value = self.value(x)\n",
    "\n",
    "        scale = 1 / math.sqrt(math.sqrt(self.input_dim)) #입력 차원에 따라 정규화 (크기 조정)\n",
    "        scores = torch.matmul(query, key.transpose(-2, -1)) * scale #Q X K_t 계산 => 결과 : 각 위치간 유사도 행렬\n",
    "        #코드는그냥 마지막 2 층을 서로 바꿔버린다는 의미\n",
    "        attention_weights = torch.softmax(scores, dim=-1) #어텐션 가중치를 소프트맥스에 따라 정규화\n",
    "\n",
    "        attended_values = torch.matmul(attention_weights, value) #어텐션 가중치에 V를 곱하여 중요 정보를 반영한 결과 생성 \n",
    "        output = attended_values + x #잔차 연결\n",
    "\n",
    "        return output, attention_weights #어텐션 가중치값과 잔차연결 된 이미지 출력\n",
    "\n",
    "# 코드 자체로는 일반적인 어텐션이랑 다를게 없는 듯\n",
    "# 중요하게 판단할 점은 Q,K,V 가 선형 레이어를 거쳐서 만들어진다는거. 즉 학습된다\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    r\"\"\"LayerNorm that supports two data formats: channels_last (default) or channels_first.\n",
    "    The ordering of the dimensions in the inputs. channels_last corresponds to inputs with\n",
    "    shape (batch_size, height, width, channels) while channels_first corresponds to inputs\n",
    "    with shape (batch_size, channels, height, width).\n",
    "    \"\"\" # 해당 LayerNorm은 탠서가 마지막에 있는 포맷이든, (B,H,W,C) 앞에 있는 포맷이든 둘다 처리 가능하다\n",
    "\n",
    "    def __init__(self, normalized_shape, eps=1e-6, data_format=\"channels_last\"):\n",
    "        #정규화할 차원의 크기를 입력받으며 초기화 \n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(normalized_shape)) # 정규화 출력에 곱해질 가중치 설정 (초기값 1)\n",
    "        self.bias = nn.Parameter(torch.zeros(normalized_shape)) #정규화 출력에 더해질 바이어스 설정 (초기값 0)\n",
    "        self.eps = eps #매우 작은 크기의 숫자 (앱실론) 설정\n",
    "        self.data_format = data_format #입력 데이터 포맷 설정\n",
    "        if self.data_format not in [\"channels_last\", \"channels_first\"]:\n",
    "            # 정해진 입력 데이터 포맷을 따르지 않는 데이터가 입력되었을 경우\n",
    "            raise NotImplementedError #예외 발생\n",
    "        self.normalized_shape = (normalized_shape,) #튜플의 형태로 저장\n",
    "\n",
    "    def forward(self, x): #데이터 포맷의 형태가 Channels_last라면 \n",
    "        if self.data_format == \"channels_last\": # (N,H,W,C)\n",
    "            return F.layer_norm(\n",
    "                x, self.normalized_shape, self.weight, self.bias, self.eps\n",
    "            ) #Pytorch의 내장 F.Layer_Norm으로 정규화 수행\n",
    "        elif self.data_format == \"channels_first\": #Channel_First라면, (N,C,H,W)\n",
    "            u = x.mean(1, keepdim=True) #채널 방향 (dim =1)로 평균값 계산 : 2번째 차원이여서 1\n",
    "            s = (x - u).pow(2).mean(1, keepdim=True) #분산 계산\n",
    "            x = (x - u) / torch.sqrt(s + self.eps) #입력 데이터의 정규화 수행\n",
    "            x = self.weight[:, None, None] * x + self.bias[:, None, None] #가중치와 바이어스를 곱하고 더하기\n",
    "            return x\n",
    "\n",
    "    # 왜 따로 이렇게 만들었을까? Pytorch의 내장 함수 쓰지않고?\n",
    "    # 유지보수성이라든가, 성능 향상적 측면에서 효율적일 수 있기 때문\n",
    "    \n",
    "class Block(nn.Module):\n",
    "    r\"\"\"ConvNeXt Block. There are two equivalent implementations:\n",
    "    (1) DwConv -> LayerNorm (channels_first) -> 1x1 Conv -> GELU -> 1x1 Conv; all in (N, C, H, W)\n",
    "    (2) DwConv -> Permute to (N, H, W, C); LayerNorm (channels_last) -> Linear -> GELU -> Linear; Permute back\n",
    "    We use (2) as we find it slightly faster in PyTorch\n",
    "\n",
    "    Args:\n",
    "        dim (int): Number of input channels.\n",
    "        drop_path (float): Stochastic depth rate. Default: 0.0\n",
    "        layer_scale_init_value (float): Init value for Layer Scale. Default: 1e-6.\n",
    "    \"\"\"\n",
    "    #ConvNeXt 블록 : 2가지 방식으로 동일하게 구현이 가능하다.\n",
    "    #DwConv -> LayerNorm (Channels_First) -> 1x1 Conv -> GELU -> 1x1 Conv : 모든 연산을 (N,C,H,W)포맷에서 수행\n",
    "    #DwConv -> (N, H, W, C) 형태로 변환 -> LayerNorm (channels_last) -> Linear -> GELU -> Linear -> 다시 원래 형태로 변환\n",
    "    #2번 방식이 좀 더 빨라서 2번을 사용한다\n",
    "    \n",
    "    #궁금 1. 왜 2번은 Linear로 Conv로 대체했을까?\n",
    "    #1x1 Conv는 공간을 건드리지 않고 채널 간 변환만 하므로 (H,W)는 그대로 두되 C만 변환하는 Linear과 동일하게 작동할 수 있다\n",
    "\n",
    "    def __init__(self, dim, drop_path=0.0, layer_scale_init_value=1e-6):\n",
    "        super().__init__() \n",
    "        #dim = 입력 채널 수, Stochastic Depth 비율 : 블록을 확률적으로 생략하는 비율, Local_Scale 초기값\n",
    "        self.dwconv = nn.Conv2d(\n",
    "            dim, dim, kernel_size=7, padding=3, groups=dim\n",
    "        )  # depthwise conv : 채널마다 독립적인 Convolution 수행, 7x7커널을 사용하며 채널수는 유지, 출력 크기를 입력과 동일하게 유지\n",
    "        # Depthwise Conv란? Xception에서 제시되었던 컨셉의 Conv, 채널별로 독립적인 Conv연산을 수행한다\n",
    "        # 결과적인 채널 수는 동일하다\n",
    "        self.norm = LayerNorm(dim, eps=1e-6) # 입력 텐서의 채널 방향에 대해 레이어 정규화 수행 \n",
    "        self.pwconv1 = nn.Linear(\n",
    "            dim, 4 * dim\n",
    "        )  # pointwise/1x1 convs, implemented with linear layers : Pointwise 1x1 Conv 수행\n",
    "        self.act = nn.GELU() #비선형 활성화 GELU 적용\n",
    "        self.pwconv2 = nn.Linear(4 * dim, dim) #다시 Pointwise 1x1 Conv 수행 (원래대로 축소)\n",
    "        self.gamma = (\n",
    "            nn.Parameter(layer_scale_init_value * torch.ones((dim)), requires_grad=True)\n",
    "            if layer_scale_init_value > 0\n",
    "            else None\n",
    "        )#dim 차원만큼 각 채널에 Scale을 곱, 아주 작은 값에서 시작하여 학습하면서 커진다\n",
    "        #이는 잔차블록의 역할\n",
    "        self.stochastic_depth = StochasticDepth(drop_path, \"row\")\n",
    "        # drop_path의 확률에 따라 블록을 통째로 skip할 수 있다\n",
    "        #(= 배치 단위가 아닌 sample(행)단위로 drop 여부를 결정한다\n",
    "\n",
    "    def forward(self, x): # 순전파 기준\n",
    "        input = x # 입력 데이터 x\n",
    "        x = self.dwconv(x) # 1x1 Conv\n",
    "        x = x.permute(0, 2, 3, 1)  # (N, C, H, W) -> (N, H, W, C)\n",
    "        x = self.norm(x) \n",
    "        x = self.pwconv1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.pwconv2(x)\n",
    "        if self.gamma is not None: # \n",
    "            x = self.gamma * x\n",
    "        x = x.permute(0, 3, 1, 2)  # (N, H, W, C) -> (N, C, H, W)\n",
    "\n",
    "        x = input + self.stochastic_depth(x)\n",
    "        return x\n",
    "\n",
    "class EmoNeXt(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_chans=3, # 입력 이미지의 채널 수\n",
    "        num_classes=1000, #출력 클래스의 수 = 1000개\n",
    "        depths=None, # 각 stage에서의 블록의 개수를 정의\n",
    "        dims=None, # 각 stage에서의 차원 크기 리스트\n",
    "        drop_path_rate=0.0, # Stochastic Depth를 적용할 확률\n",
    "        layer_scale_init_value=1e-6, # LayersCALE 시의 초기값\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        if dims is None:\n",
    "            dims = [96, 192, 384, 768]\n",
    "        if depths is None:\n",
    "            depths = [3, 3, 9, 3]\n",
    "\n",
    "        # localization-network 정의\n",
    "        self.localization = nn.Sequential(\n",
    "            nn.Conv2d(3, 8, kernel_size=7),\n",
    "            nn.BatchNorm2d(8),\n",
    "            nn.MaxPool2d(2, stride=2),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(8, 10, kernel_size=5),\n",
    "            nn.BatchNorm2d(10),\n",
    "            nn.MaxPool2d(2, stride=2),\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    "\n",
    "        # Regressor for the 3 * 2 affine matrix\n",
    "        self.fc_loc = nn.Sequential(\n",
    "            nn.Linear(10 * 52 * 52, 32), nn.ReLU(True), nn.Linear(32, 3 * 2)\n",
    "        )\n",
    "        # STN 과정(Affine 변환)을 위한 변환 행렬 theta를 예측하는 Localization Head\n",
    "        \n",
    "        self.downsample_layers = (\n",
    "            nn.ModuleList()\n",
    "        )  # 여러 스테이지를 담을 모듈 리스트 선언\n",
    "        \n",
    "        stem = nn.Sequential(\n",
    "            nn.Conv2d(in_chans, dims[0], kernel_size=4, stride=4),\n",
    "            LayerNorm(dims[0], eps=1e-6, data_format=\"channels_first\"),\n",
    "        ) # 초기 해상도 감소 및 채널 정규화\n",
    "        \n",
    "        self.downsample_layers.append(stem) #stage로 추가\n",
    "        \n",
    "        for i in range(3):\n",
    "            downsample_layer = nn.Sequential(\n",
    "                LayerNorm(dims[i], eps=1e-6, data_format=\"channels_first\"), #이전 출력 정규화\n",
    "                nn.Conv2d(dims[i], dims[i + 1], kernel_size=2, stride=2), #채널 수 증가\n",
    "                SE_Layer(dims[i + 1]), #채널의 어텐션 매커니증 적용\n",
    "            )\n",
    "            self.downsample_layers.append(downsample_layer) #stage 추가\n",
    "\n",
    "        self.stages = (\n",
    "            nn.ModuleList()\n",
    "        )  #스테이지를 담을 모듈리스트를 선언 2\n",
    "        dp_rates = [x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))]\n",
    "        #전체 블록 수 만큼 Stochastic Depth 확률을 고르게 생성한다\n",
    "        # 깊은 블록일수록 더 자주 skip 되게 설정 -> 왜? 과적합 방지와 학습 안정화의 효력\n",
    "        cur = 0\n",
    "        for i in range(4):\n",
    "            stage = nn.Sequential(\n",
    "                *[\n",
    "                    Block(\n",
    "                        dim=dims[i], # 현재 stage의 채널 수\n",
    "                        drop_path=dp_rates[cur + j], # 현재 Block에 해당하는 drop_path 확률\n",
    "                        layer_scale_init_value=layer_scale_init_value, #초기 값\n",
    "                    )\n",
    "                    for j in range(depths[i]) #각 stage 별로 블록 개수 설정해서 stage 하나로 만든다\n",
    "                ]\n",
    "            )\n",
    "            self.stages.append(stage)\n",
    "            cur += depths[i] #다음 stage의 dp_rates 인덱스를 맞추기 위해 현재 Block 수만큼 더하기\n",
    "            \n",
    "        # 96채널 3개 192채널 3개 384채널 9개 768채널 3개 로 구설\n",
    "        # 점점 고차원의 특징을 학습한다    \n",
    "        \n",
    "        #최종 채널 축[-1]에 대해 정규화와 어텐션 수행\n",
    "        self.norm = nn.LayerNorm(dims[-1], eps=1e-6)  # final norm layer\n",
    "        self.attention = DotProductSelfAttention(dims[-1])\n",
    "        self.head = nn.Linear(dims[-1], num_classes)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, (nn.Conv2d, nn.Linear)):\n",
    "                nn.init.trunc_normal_(m.weight, std=0.02)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "                    \n",
    "        # 모델 내의 모든 서브 모듈을 순회하여 가중치 초기화와 바이어스 초기화 진행\n",
    "\n",
    "        self.fc_loc[2].weight.data.zero_()\n",
    "        self.fc_loc[2].bias.data.copy_(\n",
    "            torch.tensor([1, 0, 0, 0, 1, 0], dtype=torch.float)\n",
    "        # STN 변환을 위한 Affine 변환 행렬 초기화\n",
    "        # STN에서는 항등 행렬을 사용하여 처음에 아무 변화 없이 이미지가 전달되도록 보장된다\n",
    "        )\n",
    "        \n",
    "        #해당 초기화는 모델 실행 시에 한번만 실행되는 초기화 코드\n",
    "\n",
    "    def stn(self, x): #STN : 이미지를 적다하게 조절하여 잘 인식하게끔 만드는 과정\n",
    "        xs = self.localization(x) #Localization으로 특징맵을 추출\n",
    "        xs = xs.view(-1, 10 * 52 * 52) # 2D텐서로 펼침\n",
    "        theta = self.fc_loc(xs) # 변환행렬 Theta를 예측하여 대입\n",
    "        theta = theta.view(-1, 2, 3)\n",
    " \n",
    "        grid = F.affine_grid(theta, x.size(), align_corners=True) #STN을 위해 이미지를 변환시킬 픽셀위치좌표계 생성\n",
    "        x = F.grid_sample(x, grid, align_corners=True) # 만들어진 grid에 따라 입력 이미지 이동(샘플링 및 보간) \n",
    "\n",
    "        return x\n",
    "\n",
    "    def forward_features(self, x): #순방향 처리 과정\n",
    "        for i in range(4):\n",
    "            x = self.downsample_layers[i](x)\n",
    "            x = self.stages[i](x)\n",
    "        return self.norm(\n",
    "            x.mean([-2, -1]) \n",
    "        )  # global average pooling, (N, C, H, W) -> (N, C)\n",
    "        \n",
    "    # 최종 결과물은 특징벡터를 뽑아낸다\n",
    "\n",
    "    def forward(self, x, labels=None): #순방향\n",
    "        x = self.stn(x) #STN 과정\n",
    "        x = self.forward_features(x) #특징 추출\n",
    "        _, weights = self.attention(x) # 가중치 추출\n",
    "        logits = self.head(x) #확률기반 예측\n",
    "\n",
    "        if labels is not None: #정답이 있으면 Loss 계산\n",
    "            mean_attention_weight = torch.mean(weights)\n",
    "            attention_loss = torch.mean((weights - mean_attention_weight) ** 2)\n",
    "\n",
    "            loss = F.cross_entropy(logits, labels, label_smoothing=0.2) + attention_loss\n",
    "            return torch.argmax(logits, dim=1), logits, loss\n",
    "        return torch.argmax(logits, dim=1), logits\n",
    "\n",
    "def get_model(num_classes, model_size=\"tiny\", in_22k=False):\n",
    "    # num_classes : 최종 출력 클래스 수 (감정 수)\n",
    "    if model_size == \"tiny\":\n",
    "        depths = [3, 3, 9, 3]\n",
    "        dims = [96, 192, 384, 768]\n",
    "        url = (\n",
    "            model_urls[\"convnext_tiny_22k\"]\n",
    "            if in_22k\n",
    "            else model_urls[\"convnext_tiny_1k\"]\n",
    "        )#url은 사전학습 weight를 다운로드할\n",
    "    elif model_size == \"small\":\n",
    "        depths = [3, 3, 27, 3]\n",
    "        dims = [96, 192, 384, 768]\n",
    "        url = (\n",
    "            model_urls[\"convnext_small_22k\"]\n",
    "            if in_22k\n",
    "            else model_urls[\"convnext_small_1k\"]\n",
    "        )\n",
    "    elif model_size == \"base\":\n",
    "        depths = [3, 3, 27, 3]\n",
    "        dims = [128, 256, 512, 1024]\n",
    "        url = (\n",
    "            model_urls[\"convnext_base_22k\"]\n",
    "            if in_22k\n",
    "            else model_urls[\"convnext_base_1k\"]\n",
    "        )\n",
    "    elif model_size == \"large\":\n",
    "        depths = [3, 3, 27, 3]\n",
    "        dims = [192, 384, 768, 1536]\n",
    "        url = (\n",
    "            model_urls[\"convnext_large_22k\"]\n",
    "            if in_22k\n",
    "            else model_urls[\"convnext_large_1k\"]\n",
    "        )\n",
    "    else:\n",
    "        depths = [3, 3, 27, 3]\n",
    "        dims = [256, 512, 1024, 2048]\n",
    "        url = model_urls[\"convnext_xlarge_22k\"]\n",
    "\n",
    "    default_num_classes = 1000\n",
    "    if in_22k:\n",
    "        default_num_classes = 21841\n",
    "\n",
    "    net = EmoNeXt(\n",
    "        depths=depths, dims=dims, num_classes=default_num_classes, drop_path_rate=0.1\n",
    "    )\n",
    "\n",
    "    state_dict = load_state_dict_from_url(url=url)\n",
    "    net.load_state_dict(state_dict[\"model\"], strict=False)\n",
    "    net.head = nn.Linear(dims[-1], num_classes)\n",
    "\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a19a5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6b975931",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ema_pytorch in c:\\users\\hyi8402\\anaconda3\\envs\\fer-env\\lib\\site-packages (0.7.7)\n",
      "Requirement already satisfied: torch>=2.0 in c:\\users\\hyi8402\\anaconda3\\envs\\fer-env\\lib\\site-packages (from ema_pytorch) (2.7.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\hyi8402\\anaconda3\\envs\\fer-env\\lib\\site-packages (from torch>=2.0->ema_pytorch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\hyi8402\\anaconda3\\envs\\fer-env\\lib\\site-packages (from torch>=2.0->ema_pytorch) (4.14.1)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\hyi8402\\anaconda3\\envs\\fer-env\\lib\\site-packages (from torch>=2.0->ema_pytorch) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\hyi8402\\anaconda3\\envs\\fer-env\\lib\\site-packages (from torch>=2.0->ema_pytorch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\hyi8402\\anaconda3\\envs\\fer-env\\lib\\site-packages (from torch>=2.0->ema_pytorch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\users\\hyi8402\\anaconda3\\envs\\fer-env\\lib\\site-packages (from torch>=2.0->ema_pytorch) (2025.5.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\hyi8402\\anaconda3\\envs\\fer-env\\lib\\site-packages (from sympy>=1.13.3->torch>=2.0->ema_pytorch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\hyi8402\\anaconda3\\envs\\fer-env\\lib\\site-packages (from jinja2->torch>=2.0->ema_pytorch) (3.0.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: tqdm in c:\\users\\hyi8402\\anaconda3\\envs\\fer-env\\lib\\site-packages (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\hyi8402\\anaconda3\\envs\\fer-env\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: wandb in c:\\users\\hyi8402\\anaconda3\\envs\\fer-env\\lib\\site-packages (0.21.0)\n",
      "Requirement already satisfied: click!=8.0.0,>=7.1 in c:\\users\\hyi8402\\anaconda3\\envs\\fer-env\\lib\\site-packages (from wandb) (8.2.1)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in c:\\users\\hyi8402\\anaconda3\\envs\\fer-env\\lib\\site-packages (from wandb) (3.1.44)\n",
      "Requirement already satisfied: packaging in c:\\users\\hyi8402\\anaconda3\\envs\\fer-env\\lib\\site-packages (from wandb) (25.0)\n",
      "Requirement already satisfied: platformdirs in c:\\users\\hyi8402\\anaconda3\\envs\\fer-env\\lib\\site-packages (from wandb) (4.3.8)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in c:\\users\\hyi8402\\anaconda3\\envs\\fer-env\\lib\\site-packages (from wandb) (5.29.5)\n",
      "Requirement already satisfied: pydantic<3 in c:\\users\\hyi8402\\anaconda3\\envs\\fer-env\\lib\\site-packages (from wandb) (2.11.7)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\hyi8402\\anaconda3\\envs\\fer-env\\lib\\site-packages (from wandb) (6.0.2)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in c:\\users\\hyi8402\\anaconda3\\envs\\fer-env\\lib\\site-packages (from wandb) (2.32.4)\n",
      "Requirement already satisfied: sentry-sdk>=2.0.0 in c:\\users\\hyi8402\\anaconda3\\envs\\fer-env\\lib\\site-packages (from wandb) (2.33.0)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.8 in c:\\users\\hyi8402\\anaconda3\\envs\\fer-env\\lib\\site-packages (from wandb) (4.14.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\hyi8402\\anaconda3\\envs\\fer-env\\lib\\site-packages (from pydantic<3->wandb) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\hyi8402\\anaconda3\\envs\\fer-env\\lib\\site-packages (from pydantic<3->wandb) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\hyi8402\\anaconda3\\envs\\fer-env\\lib\\site-packages (from pydantic<3->wandb) (0.4.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\hyi8402\\anaconda3\\envs\\fer-env\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hyi8402\\anaconda3\\envs\\fer-env\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\hyi8402\\anaconda3\\envs\\fer-env\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hyi8402\\anaconda3\\envs\\fer-env\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (2025.6.15)\n",
      "Requirement already satisfied: colorama in c:\\users\\hyi8402\\anaconda3\\envs\\fer-env\\lib\\site-packages (from click!=8.0.0,>=7.1->wandb) (0.4.6)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in c:\\users\\hyi8402\\anaconda3\\envs\\fer-env\\lib\\site-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in c:\\users\\hyi8402\\anaconda3\\envs\\fer-env\\lib\\site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install ema_pytorch\n",
    "%pip install tqdm\n",
    "%pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19168f6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(dataset_path='C:\\\\Users\\\\hyi8402\\\\Desktop\\\\실시간 얼굴 감정 검출\\\\DataSet\\\\FER2013', output_dir='out', epochs=2, batch_size=16, lr=0.0001, amp=False, in_22k=False, gradient_accumulation_steps=1, num_workers=0, checkpoint=None, model_size='tiny')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing previous runs because reinit is set to 'default'."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">EmoNeXt_tiny_2025-07-21 10:59:13</strong> at: <a href='https://wandb.ai/anony-moose-483922537331977471/EmoNeXt/runs/f4790a0y?apiKey=903b9e7544e6acaaf721345358251648b08b6b9c' target=\"_blank\">https://wandb.ai/anony-moose-483922537331977471/EmoNeXt/runs/f4790a0y?apiKey=903b9e7544e6acaaf721345358251648b08b6b9c</a><br> View project at: <a href='https://wandb.ai/anony-moose-483922537331977471/EmoNeXt?apiKey=903b9e7544e6acaaf721345358251648b08b6b9c' target=\"_blank\">https://wandb.ai/anony-moose-483922537331977471/EmoNeXt?apiKey=903b9e7544e6acaaf721345358251648b08b6b9c</a><br>Synced 5 W&B file(s), 18 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250721_105913-f4790a0y\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\hyi8402\\Desktop\\실시간 얼굴 감정 검출\\wandb\\run-20250721_105923-bj6htzor</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/anony-moose-483922537331977471/EmoNeXt/runs/bj6htzor?apiKey=903b9e7544e6acaaf721345358251648b08b6b9c' target=\"_blank\">EmoNeXt_tiny_2025-07-21 10:59:23</a></strong> to <a href='https://wandb.ai/anony-moose-483922537331977471/EmoNeXt?apiKey=903b9e7544e6acaaf721345358251648b08b6b9c' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/anony-moose-483922537331977471/EmoNeXt?apiKey=903b9e7544e6acaaf721345358251648b08b6b9c' target=\"_blank\">https://wandb.ai/anony-moose-483922537331977471/EmoNeXt?apiKey=903b9e7544e6acaaf721345358251648b08b6b9c</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/anony-moose-483922537331977471/EmoNeXt/runs/bj6htzor?apiKey=903b9e7544e6acaaf721345358251648b08b6b9c' target=\"_blank\">https://wandb.ai/anony-moose-483922537331977471/EmoNeXt/runs/bj6htzor?apiKey=903b9e7544e6acaaf721345358251648b08b6b9c</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Do NOT share these links with anyone. They can be used to claim your runs."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 28709 images for training.\n",
      "Using 7178 images for evaluation.\n",
      "Using 7178 images for testing.\n",
      "[Train Epoch 1/2]:   1%|          | 51/7178 [01:55<4:29:40,  2.27s/batch, loss=1.97, acc=18.1]\n",
      "[Train Epoch 1/2]:   0%|          | 2/1795 [00:27<6:53:03, 13.82s/batch, loss=1.95, acc=6.25]\n",
      "Device used: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hyi8402\\AppData\\Local\\Temp\\ipykernel_8128\\2275324683.py:66: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(enabled=self.amp)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch: 1/2]\n",
      "[Train Epoch 1/2]:   3%|▎         | 49/1795 [03:37<2:15:07,  4.64s/batch, loss=1.93, acc=23.3]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[40], line 396\u001b[0m\n\u001b[0;32m    381\u001b[0m \u001b[38;5;66;03m# ---------- Trainer ----------\u001b[39;00m\n\u001b[0;32m    382\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[0;32m    383\u001b[0m     model\u001b[38;5;241m=\u001b[39mnet,\n\u001b[0;32m    384\u001b[0m     training_dataloader\u001b[38;5;241m=\u001b[39mtrain_loader,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    393\u001b[0m     amp\u001b[38;5;241m=\u001b[39mopt\u001b[38;5;241m.\u001b[39mamp,\n\u001b[0;32m    394\u001b[0m )\n\u001b[1;32m--> 396\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[40], line 100\u001b[0m, in \u001b[0;36mTrainer.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Epoch: \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepochs))\n\u001b[0;32m     99\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvisualize_stn()\n\u001b[1;32m--> 100\u001b[0m train_loss, train_accuracy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    101\u001b[0m val_loss, val_accuracy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mval_epoch()\n\u001b[0;32m    103\u001b[0m wandb\u001b[38;5;241m.\u001b[39mlog(\n\u001b[0;32m    104\u001b[0m     {\n\u001b[0;32m    105\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrain Loss\u001b[39m\u001b[38;5;124m\"\u001b[39m: train_loss,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    110\u001b[0m     }\n\u001b[0;32m    111\u001b[0m )\n",
      "Cell \u001b[1;32mIn[40], line 151\u001b[0m, in \u001b[0;36mTrainer.train_epoch\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    148\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautocast(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;241m.\u001b[39mtype, enabled\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mamp):\n\u001b[0;32m    149\u001b[0m     predictions, _, loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(inputs, labels)\n\u001b[1;32m--> 151\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    152\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (batch_idx \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    153\u001b[0m     torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mparameters(), \u001b[38;5;241m1.0\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\hyi8402\\anaconda3\\envs\\fer-env\\lib\\site-packages\\torch\\_tensor.py:648\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    638\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    639\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    640\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    641\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    646\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    647\u001b[0m     )\n\u001b[1;32m--> 648\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    649\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    650\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\hyi8402\\anaconda3\\envs\\fer-env\\lib\\site-packages\\torch\\autograd\\__init__.py:353\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    348\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    350\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    351\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    352\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 353\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    356\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    357\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    358\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    359\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    360\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    361\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\hyi8402\\anaconda3\\envs\\fer-env\\lib\\site-packages\\torch\\autograd\\graph.py:824\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    822\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    823\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 824\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    825\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    826\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    827\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    828\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "File \u001b[1;32mc:\\Users\\hyi8402\\anaconda3\\envs\\fer-env\\lib\\site-packages\\wandb\\integration\\torch\\wandb_torch.py:276\u001b[0m, in \u001b[0;36mTorchHistory._hook_variable_gradient_stats.<locals>.<lambda>\u001b[1;34m(grad)\u001b[0m\n\u001b[0;32m    273\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m    274\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_tensor_stats(grad\u001b[38;5;241m.\u001b[39mdata, name)\n\u001b[1;32m--> 276\u001b[0m handle \u001b[38;5;241m=\u001b[39m var\u001b[38;5;241m.\u001b[39mregister_hook(\u001b[38;5;28;01mlambda\u001b[39;00m grad: _callback(grad, log_track))\n\u001b[0;32m    277\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_hook_handles[name] \u001b[38;5;241m=\u001b[39m handle\n\u001b[0;32m    278\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m handle\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import random\n",
    "import sys\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "from ema_pytorch import EMA\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from tqdm import tqdm\n",
    "\n",
    "import wandb\n",
    "\n",
    "# 경로 자동 지정\n",
    "if len(sys.argv) == 1:\n",
    "    sys.argv.extend([\n",
    "        \"--dataset-path\", r\"C:\\Users\\hyi8402\\Desktop\\실시간 얼굴 감정 검출\\DataSet\\FER2013\"\n",
    "    ])\n",
    "# -------------------\n",
    "class Trainer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model,\n",
    "        training_dataloader,\n",
    "        validation_dataloader,\n",
    "        testing_dataloader,\n",
    "        classes,\n",
    "        output_dir,\n",
    "        max_epochs: int = 10000,\n",
    "        early_stopping_patience: int = 12,\n",
    "        execution_name=None,\n",
    "        lr: float = 1e-4,\n",
    "        amp: bool = False,\n",
    "        ema_decay: float = 0.99,\n",
    "        ema_update_every: int = 16,\n",
    "        gradient_accumulation_steps: int = 1,\n",
    "        checkpoint_path: str = None,\n",
    "    ):\n",
    "        self.epochs = max_epochs\n",
    "\n",
    "        self.training_dataloader = training_dataloader\n",
    "        self.validation_dataloader = validation_dataloader\n",
    "        self.testing_dataloader = testing_dataloader\n",
    "\n",
    "        self.classes = classes\n",
    "        self.num_classes = len(classes)\n",
    "\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        print(\"Device used: \" + self.device.type)\n",
    "\n",
    "        self.amp = amp\n",
    "        self.gradient_accumulation_steps = gradient_accumulation_steps\n",
    "\n",
    "        self.model = model.to(self.device)\n",
    "\n",
    "        self.optimizer = AdamW(model.parameters(), lr=lr)\n",
    "        self.scaler = torch.cuda.amp.GradScaler(enabled=self.amp)\n",
    "        self.scheduler = CosineAnnealing_WarmRestartsLR(\n",
    "            self.optimizer, warmup_steps=10, cycle_steps=50\n",
    "        )\n",
    "        self.ema = EMA(model, beta=ema_decay, update_every=ema_update_every).to(\n",
    "            self.device\n",
    "        )\n",
    "\n",
    "        self.early_stopping_patience = early_stopping_patience\n",
    "\n",
    "        self.output_directory = Path(output_dir)\n",
    "        self.output_directory.mkdir(exist_ok=True)\n",
    "\n",
    "        self.best_val_accuracy = 0\n",
    "\n",
    "        self.execution_name = \"model\" if execution_name is None else execution_name\n",
    "\n",
    "        if checkpoint_path:\n",
    "            self.load(checkpoint_path)\n",
    "\n",
    "        wandb.watch(model, log=\"all\")\n",
    "\n",
    "    def run(self):\n",
    "        counter = 0  # Counter for epochs with no validation loss improvement\n",
    "\n",
    "        images, _ = next(iter(self.training_dataloader))\n",
    "        images = [transforms.ToPILImage()(image) for image in images]\n",
    "        wandb.log({\"Images\": [wandb.Image(image) for image in images]})\n",
    "\n",
    "        for epoch in range(self.epochs):\n",
    "            self.current_epoch = epoch\n",
    "            print(\"[Epoch: %d/%d]\" % (epoch + 1, self.epochs))\n",
    "\n",
    "            self.visualize_stn()\n",
    "            train_loss, train_accuracy = self.train_epoch()\n",
    "            val_loss, val_accuracy = self.val_epoch()\n",
    "\n",
    "            wandb.log(\n",
    "                {\n",
    "                    \"Train Loss\": train_loss,\n",
    "                    \"Val Loss\": val_loss,\n",
    "                    \"Train Accuracy\": train_accuracy,\n",
    "                    \"Val Accuracy\": val_accuracy,\n",
    "                    \"Epoch\": epoch + 1,\n",
    "                }\n",
    "            )\n",
    "\n",
    "            # Early stopping\n",
    "            if val_accuracy > self.best_val_accuracy:\n",
    "                self.save()\n",
    "                counter = 0\n",
    "                self.best_val_accuracy = val_accuracy\n",
    "            else:\n",
    "                counter += 1\n",
    "                if counter >= self.early_stopping_patience:\n",
    "                    print(\n",
    "                        \"Validation loss did not improve for %d epochs. Stopping training.\"\n",
    "                        % self.early_stopping_patience\n",
    "                    )\n",
    "                    break\n",
    "\n",
    "        self.test_model()\n",
    "        wandb.finish()\n",
    "\n",
    "    def train_epoch(self):\n",
    "        self.model.train()\n",
    "\n",
    "        avg_accuracy = []\n",
    "        avg_loss = []\n",
    "\n",
    "        pbar = tqdm(\n",
    "            unit=\"batch\",\n",
    "            file=sys.stdout,\n",
    "            total=len(self.training_dataloader),\n",
    "            desc=f\"[Train Epoch {self.current_epoch+1}/{self.epochs}]\"\n",
    "        )\n",
    "        for batch_idx, data in enumerate(self.training_dataloader):\n",
    "            inputs, labels = data\n",
    "\n",
    "            inputs = inputs.to(self.device)\n",
    "            labels = labels.to(self.device)\n",
    "\n",
    "            with torch.autocast(self.device.type, enabled=self.amp):\n",
    "                predictions, _, loss = self.model(inputs, labels)\n",
    "\n",
    "            self.scaler.scale(loss).backward()\n",
    "            if (batch_idx + 1) % self.gradient_accumulation_steps == 0:\n",
    "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n",
    "                self.scaler.step(self.optimizer)\n",
    "                self.optimizer.zero_grad(set_to_none=True)\n",
    "                self.scaler.update()\n",
    "                self.ema.update()\n",
    "                self.scheduler.step()\n",
    "\n",
    "            batch_accuracy = (predictions == labels).sum().item() / labels.size(0)\n",
    "\n",
    "            avg_loss.append(loss.item())\n",
    "            avg_accuracy.append(batch_accuracy)\n",
    "\n",
    "            # Update progress bar\n",
    "            pbar.set_postfix(\n",
    "                {\"loss\": np.mean(avg_loss), \"acc\": np.mean(avg_accuracy) * 100.0}\n",
    "            )\n",
    "            pbar.update(1)\n",
    "\n",
    "        pbar.close()\n",
    "\n",
    "        return np.mean(avg_loss), np.mean(avg_accuracy) * 100.0\n",
    "\n",
    "    def val_epoch(self):\n",
    "        self.model.eval()\n",
    "\n",
    "        avg_loss = []\n",
    "        predicted_labels = []\n",
    "        true_labels = []\n",
    "\n",
    "        pbar = tqdm(\n",
    "            unit=\"batch\", file=sys.stdout, total=len(self.validation_dataloader)\n",
    "        )\n",
    "        for batch_idx, (inputs, labels) in enumerate(self.validation_dataloader):\n",
    "            inputs = inputs.to(self.device)\n",
    "            labels = labels.to(self.device)\n",
    "\n",
    "            with torch.autocast(self.device.type, enabled=self.amp):\n",
    "                predictions, _, loss = self.model(inputs, labels)\n",
    "\n",
    "            avg_loss.append(loss.item())\n",
    "            predicted_labels.extend(predictions.tolist())\n",
    "            true_labels.extend(labels.tolist())\n",
    "\n",
    "            pbar.update(1)\n",
    "\n",
    "        pbar.close()\n",
    "\n",
    "        accuracy = (\n",
    "            torch.eq(torch.tensor(predicted_labels), torch.tensor(true_labels))\n",
    "            .float()\n",
    "            .mean()\n",
    "            .item()\n",
    "        )\n",
    "        wandb.log(\n",
    "            {\n",
    "                \"confusion_matrix\": wandb.plot.confusion_matrix(\n",
    "                    probs=None,\n",
    "                    y_true=true_labels,\n",
    "                    preds=predicted_labels,\n",
    "                    class_names=self.classes,\n",
    "                )\n",
    "            }\n",
    "        )\n",
    "\n",
    "        print(\n",
    "            \"Eval loss: %.4f, Eval Accuracy: %.4f %%\"\n",
    "            % (np.mean(avg_loss) * 1.0, accuracy * 100.0)\n",
    "        )\n",
    "        return np.mean(avg_loss), accuracy * 100.0\n",
    "\n",
    "    def test_model(self):\n",
    "        self.ema.eval()\n",
    "\n",
    "        predicted_labels = []\n",
    "        true_labels = []\n",
    "\n",
    "        pbar = tqdm(unit=\"batch\", file=sys.stdout, total=len(self.testing_dataloader))\n",
    "        for batch_idx, (inputs, labels) in enumerate(self.testing_dataloader):\n",
    "            bs, ncrops, c, h, w = inputs.shape\n",
    "            inputs = inputs.view(-1, c, h, w)\n",
    "\n",
    "            inputs = inputs.to(self.device)\n",
    "            labels = labels.to(self.device)\n",
    "\n",
    "            with torch.autocast(self.device.type, enabled=self.amp):\n",
    "                _, logits = self.ema(inputs)\n",
    "            outputs_avg = logits.view(bs, ncrops, -1).mean(1)\n",
    "            predictions = torch.argmax(outputs_avg, dim=1)\n",
    "\n",
    "            predicted_labels.extend(predictions.tolist())\n",
    "            true_labels.extend(labels.tolist())\n",
    "\n",
    "            pbar.update(1)\n",
    "\n",
    "        pbar.close()\n",
    "\n",
    "        accuracy = (\n",
    "            torch.eq(torch.tensor(predicted_labels), torch.tensor(true_labels))\n",
    "            .float()\n",
    "            .mean()\n",
    "            .item()\n",
    "        )\n",
    "        print(\"Test Accuracy: %.4f %%\" % (accuracy * 100.0))\n",
    "\n",
    "        wandb.log(\n",
    "            {\n",
    "                \"confusion_matrix\": wandb.plot.confusion_matrix(\n",
    "                    probs=None,\n",
    "                    y_true=true_labels,\n",
    "                    preds=predicted_labels,\n",
    "                    class_names=self.classes,\n",
    "                )\n",
    "            }\n",
    "        )\n",
    "\n",
    "    def visualize_stn(self):\n",
    "        self.model.eval()\n",
    "\n",
    "        batch = torch.utils.data.Subset(val_dataset, range(32))\n",
    "\n",
    "        # Access the batch data\n",
    "        batch = torch.stack([batch[i][0] for i in range(len(batch))]).to(self.device)\n",
    "        with torch.autocast(self.device.type, enabled=self.amp):\n",
    "            stn_batch = self.model.stn(batch)\n",
    "\n",
    "        to_pil = transforms.ToPILImage()\n",
    "\n",
    "        grid = to_pil(torchvision.utils.make_grid(batch, nrow=16, padding=4))\n",
    "        stn_batch = to_pil(torchvision.utils.make_grid(stn_batch, nrow=16, padding=4))\n",
    "\n",
    "        wandb.log({\"batch\": wandb.Image(grid), \"stn\": wandb.Image(stn_batch)})\n",
    "\n",
    "    def save(self):\n",
    "        data = {\n",
    "            \"model\": self.model.state_dict(),\n",
    "            \"opt\": self.optimizer.state_dict(),\n",
    "            \"ema\": self.ema.state_dict(),\n",
    "            \"scaler\": self.scaler.state_dict(),\n",
    "            \"scheduler\": self.scheduler.state_dict(),\n",
    "            \"best_acc\": self.best_val_accuracy,\n",
    "        }\n",
    "\n",
    "        torch.save(data, str(self.output_directory / f\"{self.execution_name}.pt\"))\n",
    "\n",
    "    def load(self, path):\n",
    "        data = torch.load(path, map_location=self.device)\n",
    "\n",
    "        self.model.load_state_dict(data[\"model\"])\n",
    "        self.optimizer.load_state_dict(data[\"opt\"])\n",
    "        self.ema.load_state_dict(data[\"ema\"])\n",
    "        self.scaler.load_state_dict(data[\"scaler\"])\n",
    "        self.scheduler.load_state_dict(data[\"scheduler\"])\n",
    "        self.best_val_accuracy = data[\"best_acc\"]\n",
    "# -------------------\n",
    "\n",
    "# ✨ main 시작\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(description=\"Train EmoNeXt on FER2013\")\n",
    "\n",
    "    parser.add_argument(\n",
    "    \"--dataset-path\",\n",
    "    type=str,\n",
    "    default=r\"C:\\Users\\hyi8402\\Desktop\\실시간 얼굴 감정 검출\\DataSet\\FER2013\",\n",
    "    help=\"Path to the dataset\"\n",
    "    )\n",
    "    parser.add_argument(\"--output-dir\", type=str, default=\"out\")\n",
    "    parser.add_argument(\"--epochs\", type=int, default=2)\n",
    "    parser.add_argument(\"--batch-size\", type=int, default=16)\n",
    "    parser.add_argument(\"--lr\", type=float, default=1e-4)\n",
    "    parser.add_argument(\"--amp\", action=\"store_true\", default=False)\n",
    "    parser.add_argument(\"--in_22k\", action=\"store_true\", default=False)\n",
    "    parser.add_argument(\"--gradient-accumulation-steps\", type=int, default=1)\n",
    "    parser.add_argument(\"--num-workers\", type=int, default=0)\n",
    "    parser.add_argument(\"--checkpoint\", type=str, default=None)\n",
    "    parser.add_argument(\"--model-size\", choices=[\"tiny\", \"small\", \"base\", \"large\", \"xlarge\"], default=\"tiny\")\n",
    "\n",
    "    opt, _ = parser.parse_known_args()\n",
    "    print(opt)\n",
    "\n",
    "    current_time = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    exec_name = f\"EmoNeXt_{opt.model_size}_{current_time}\"\n",
    "    wandb.init(project=\"EmoNeXt\", name=exec_name, anonymous=\"must\")\n",
    "\n",
    "    # ---------- Transforms ----------\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomVerticalFlip(),\n",
    "        transforms.Grayscale(),\n",
    "        transforms.Resize(236),\n",
    "        transforms.RandomRotation(degrees=20),\n",
    "        transforms.RandomCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Lambda(lambda x: x.repeat(3, 1, 1)),\n",
    "    ])\n",
    "\n",
    "    val_transform = transforms.Compose([\n",
    "        transforms.Grayscale(),\n",
    "        transforms.Resize(236),\n",
    "        transforms.RandomCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Lambda(lambda x: x.repeat(3, 1, 1)),\n",
    "    ])\n",
    "\n",
    "    test_transform = transforms.Compose([\n",
    "        transforms.Grayscale(),\n",
    "        transforms.Resize(236),\n",
    "        transforms.TenCrop(224),\n",
    "        transforms.Lambda(lambda crops: torch.stack([transforms.ToTensor()(crop) for crop in crops])),\n",
    "        transforms.Lambda(lambda crops: torch.stack([crop.repeat(3, 1, 1) for crop in crops])),\n",
    "    ])\n",
    "\n",
    "    # ---------- Dataset ----------\n",
    "    train_dataset = ImageFolder(opt.dataset_path + \"/train\", train_transform)\n",
    "    val_dataset = ImageFolder(opt.dataset_path + \"/test\", val_transform)\n",
    "    test_dataset = ImageFolder(opt.dataset_path + \"/test\", test_transform)\n",
    "\n",
    "    print(\"Using %d images for training.\" % len(train_dataset))\n",
    "    print(\"Using %d images for evaluation.\" % len(val_dataset))\n",
    "    print(\"Using %d images for testing.\" % len(test_dataset))\n",
    "    \n",
    "    # ---------- Dataloader ----------\n",
    "    train_loader = DataLoader(train_dataset, batch_size=opt.batch_size, shuffle=True, num_workers=opt.num_workers)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "    # ---------- Model ----------\n",
    "    net = get_model(len(train_dataset.classes), opt.model_size, in_22k=opt.in_22k)\n",
    "\n",
    "    # ---------- Trainer ----------\n",
    "    trainer = Trainer(\n",
    "        model=net,\n",
    "        training_dataloader=train_loader,\n",
    "        validation_dataloader=val_loader,\n",
    "        testing_dataloader=test_loader,\n",
    "        classes=train_dataset.classes,\n",
    "        execution_name=exec_name,\n",
    "        lr=opt.lr,\n",
    "        output_dir=opt.output_dir,\n",
    "        checkpoint_path=opt.checkpoint,\n",
    "        max_epochs=opt.epochs,\n",
    "        amp=opt.amp,\n",
    "    )\n",
    "\n",
    "    trainer.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a71d69ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fer-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
